[global_tags]
# Global tags can be specified here in key="value" format.
{% if telegraf_global_tags %}
{% for key, value in telegraf_tags.iteritems() %}
    {{ key }} = "{{ value }}"
{% endfor %}
{% endif %}

[agent]
  ## Default data collection interval for all inputs
{% if telegraf_agent_interval %}
  interval = "{{ telegraf_agent_interval }}"
{% endif %}

  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
{% if telegraf_agent_round_interval %}
  round_interval = {{ telegraf_agent_round_interval }}
{% endif %}

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
{% if telegraf_agent_metric_batch_size %}
  metric_batch_size = {{ telegraf_agent_metric_batch_size }}
{% endif %}

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
{% if telegraf_agent_metric_buffer_limit %}
  metric_buffer_limit = {{ telegraf_agent_metric_buffer_limit }}
{% endif %}

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
{% if telegraf_agent_collection_jitter %}
  collection_jitter = "{{ telegraf_agent_collection_jitter }}"
{% endif %}

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
{% if telegraf_agent_flush_interval %}
  flush_interval = "{{ telegraf_agent_flush_interval }}"
{% endif %}

  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
{% if telegraf_agent_flush_jitter %}
  flush_jitter = "{{ telegraf_agent_flush_jitter }}"
{% endif %}

  ## By default, precision will be set to the same timestamp order as the
  ## collection interval, with the maximum being 1s.
  ## Precision will NOT be used for service inputs, such as logparser and statsd.
  ## Valid values are "ns", "us" (or "Âµs"), "ms", "s".
{% if telegraf_agent_precision %}
  precision = "{{ telegraf_agent_precision }}"
{% endif %}

  ## Logging configuration:
  ## Run telegraf with debug log messages.
{% if telegraf_agent_debug %}
  debug = {{ telegraf_agent_debug }}
{% endif %}

  ## Run telegraf in quiet mode (error log messages only).
{% if telegraf_agent_quiet %}
  quiet = {{ telegraf_agent_quiet }}
{% endif %}

  ## Override default hostname, if empty use os.Hostname()
{% if telegraf_agent_hostname %}
  hostname = "{{ telegraf_agent_hostname }}"
{% endif %}

  ## If set to true, do no set the "host" tag in the telegraf agent.
{% if telegraf_agent_omit_hostname %}
  omit_hostname = {{ telegraf_agent_omit_hostname }}
{% endif %}

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  ## The full HTTP or UDP endpoint URL for your InfluxDB instance.
  ## Multiple urls can be specified as part of the same cluster,
  ## this means that only ONE of the urls will be written to each interval.
  # urls = ["udp://localhost:8089"] # UDP endpoint example
  #urls = ["http://localhost:8086"] # required
{% if telegraf_outputs_influxdb_urls %}
  urls = ["{{ telegraf_outputs_influxdb_urls | join('", "') }}"]
{% endif %}

  ## The target database for metrics (telegraf will create it if not exists).
{% if telegraf_outputs_influxdb_database %}
  database = "{{ telegraf_outputs_influxdb_database }}" # required
{% endif %}

  ## Retention policy to write to. Empty string writes to the default rp.
{% if telegraf_outputs_influxdb_retention_policy %}
  retention_policy = "{{ telegraf_outputs_influxdb_retention_policy }}"
{% endif %}

  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
{% if telegraf_outputs_influxdb_write_consistency %}
  write_consistency = "{{ telegraf_outputs_influxdb_write_consistency }}"
{% endif %}

  ## Write timeout (for the InfluxDB client), formatted as a string.
  ## If not provided, will default to 5s. 0s means no timeout (not recommended).
{% if telegraf_outputs_influxdb_timeout %}
  timeout = "{{ telegraf_outputs_influxdb_timeout }}"
{% endif %}
{% if telegraf_outputs_influxdb_username %}
  username = "{{ telegraf_outputs_influxdb_username }}"
{% endif %}
{% if telegraf_outputs_influxdb_password %}
  password = "{{ telegraf_outputs_influxdb_password }}"
{% endif %}
  ## Set the user agent for HTTP POSTs (can be useful for log differentiation)
{% if telegraf_outputs_influxdb_user_agent %}
  user_agent = "{{ telegraf_outputs_influxdb_user_agent }}"
{% endif %}
  ## Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
{% if telegraf_outputs_influxdb_udp_playload %}
  udp_payload = {{ telegraf_outputs_influxdb_udp_playload }}
{% endif %}

  ## Optional SSL Config
{% if telegraf_outputs_influxdb_ssl_ca %}
  ssl_ca = "{{ telegraf_outputs_influxdb_ssl_ca }}"
{% endif %}
{% if telegraf_outputs_influxdb_ssl_cert %}
  ssl_cert = "{{ telegraf_outputs_influxdb_ssl_cert }}"
{% endif %}
{% if telegraf_outputs_influxdb_ssl_key %}
  ssl_key = "{{ telegraf_outputs_influxdb_ssl_key }}"
{% endif %}
  ## Use SSL but skip chain & host verification
{% if telegraf_outputs_influxdb_insecure_skip_verify %}
  insecure_skip_verify = {{ telegraf_outputs_influxdb_insecure_skip_verify }}
{% endif %}

# # Send telegraf metrics to file(s)
# [[outputs.file]]
#   ## Files to write to, "stdout" is a specially handled file.
#   files = ["stdout", "/tmp/metrics.out"]
#
#   ## Data format to output.
#   ## Each data format has it's own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
{% if telegraf_inputs_cpu_percpu %}
  percpu = {{ telegraf_inputs_cpu_percpu }}
{% endif %}
  ## Whether to report total system cpu stats or not
{% if telegraf_inputs_cpu_totalcpu %}
  totalcpu = {{ telegraf_inputs_cpu_totalcpu }}
{% endif %}
  ## If true, collect raw CPU time metrics.
{% if telegraf_inputs_cpu_collect_cpu_time %}
  collect_cpu_time = {{ telegraf_inputs_cpu_collect_cpu_time }}
{% endif %}

# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default, telegraf gather stats for all mountpoints.
  ## Setting mountpoints will restrict the stats to the specified mountpoints.
{% if telegraf_inputs_disk_mount_points %}
  mount_points = ["{{ telegraf_inputs_disk_mount_points | join('", "') }}"]
{% endif %}

  ## Ignore some mountpoints by filesystem type. For example (dev)tmpfs (usually
  ## present on /run, /var/run, /dev/shm or /dev).
{% if telegraf_inputs_disk_ignore_fs %}
  ignore_fs = ["{{ telegraf_inputs_disk_ignore_fs | join('", "') }}"]
{% endif %}

# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
{% if telegraf_inputs_diskio_devices %}
  devices = ["{{ telegraf_inputs_diskio_devices | join('", "') }}"]
{% endif %}
  ## Uncomment the following line if you need disk serial numbers.
{% if telegraf_inputs_diskio_skip_serial_number %}
  skip_serial_number = {{ telegraf_inputs_diskio_skip_serial_number }}
{% endif %}

# Get kernel statistics from /proc/stat
[[inputs.kernel]]
  # no configuration


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Get the number of processes and group them by status
[[inputs.processes]]
  # no configuration


# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration


# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration


# # Read metrics from one or more commands that can output to stdout
# [[inputs.exec]]
#   ## Commands array
#   commands = [
#     "/tmp/test.sh",
#     "/usr/bin/mycollector --foo=bar",
#     "/tmp/collect_*.sh"
#   ]
#
#   ## Timeout for each command to complete.
#   timeout = "5s"
#
#   ## measurement name suffix (for separating different commands)
#   name_suffix = "_mycollector"
#
#   ## Data format to consume.
#   ## Each data format has it's own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Read stats about given file(s)
# [[inputs.filestat]]
#   ## Files to gather stats about.
#   ## These accept standard unix glob matching rules, but with the addition of
#   ## ** as a "super asterisk". ie:
#   ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
#   ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
#   ##   "/var/log/apache.log" -> just tail the apache log file
#   ##
#   ## See https://github.com/gobwas/glob for more examples
#   ##
#   files = ["/var/log/**.log"]
#   ## If true, read the entire file and calculate an md5 checksum.
#   md5 = false


# # Read metrics of haproxy, via socket or csv stats page
# [[inputs.haproxy]]
#   ## An array of address to gather stats about. Specify an ip on hostname
#   ## with optional port. ie localhost, 10.10.3.33:1936, etc.
#   ## Make sure you specify the complete path to the stats endpoint
#   ## ie 10.10.3.33:1936/haproxy?stats
#   #
#   ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats
#   servers = ["http://myhaproxy.com:1936/haproxy?stats"]
#   ## Or you can also use local socket
#   ## servers = ["socket:/run/haproxy/admin.sock"]


# # HTTP/HTTPS request given an address a method and a timeout
# [[inputs.http_response]]
#   ## Server address (default http://localhost)
#   address = "http://github.com"
#   ## Set response_timeout (default 5 seconds)
#   response_timeout = "5s"
#   ## HTTP Request Method
#   method = "GET"
#   ## Whether to follow redirects from the server (defaults to false)
#   follow_redirects = true
#   ## HTTP Request Headers (all values must be strings)
#   # [inputs.http_response.headers]
#   #   Host = "github.com"
#   ## Optional HTTP Request Body
#   # body = '''
#   # {'fake':'data'}
#   # '''
#
#   ## Optional SSL Config
#   # ssl_ca = "/etc/telegraf/ca.pem"
#   # ssl_cert = "/etc/telegraf/cert.pem"
#   # ssl_key = "/etc/telegraf/key.pem"
#   ## Use SSL but skip chain & host verification
#   # insecure_skip_verify = false


# # Read flattened metrics from one or more JSON HTTP endpoints
# [[inputs.httpjson]]
#   ## NOTE This plugin only reads numerical measurements, strings and booleans
#   ## will be ignored.
#
#   ## a name for the service being polled
#   name = "webserver_stats"
#
#   ## URL of each server in the service's cluster
#   servers = [
#     "http://localhost:9999/stats/",
#     "http://localhost:9998/stats/",
#   ]
#
#   ## HTTP method to use: GET or POST (case-sensitive)
#   method = "GET"
#
#   ## List of tag names to extract from top-level of JSON server response
#   # tag_keys = [
#   #   "my_tag_1",
#   #   "my_tag_2"
#   # ]
#
#   ## HTTP parameters (all values must be strings)
#   [inputs.httpjson.parameters]
#     event_type = "cpu_spike"
#     threshold = "0.75"
#
#   ## HTTP Header parameters (all values must be strings)
#   # [inputs.httpjson.headers]
#   #   X-Auth-Token = "my-xauth-token"
#   #   apiVersion = "v1"
#
#   ## Optional SSL Config
#   # ssl_ca = "/etc/telegraf/ca.pem"
#   # ssl_cert = "/etc/telegraf/cert.pem"
#   # ssl_key = "/etc/telegraf/key.pem"
#   ## Use SSL but skip chain & host verification
#   # insecure_skip_verify = false


# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints
# [[inputs.influxdb]]
#   ## Works with InfluxDB debug endpoints out of the box,
#   ## but other services can use this format too.
#   ## See the influxdb plugin's README for more details.
#
#   ## Multiple URLs from which to read InfluxDB-formatted JSON
#   ## Default is "http://localhost:8086/debug/vars".
#   urls = [
#     "http://localhost:8086/debug/vars"
#   ]


# # Read metrics about network interface usage
# [[inputs.net]]
#   ## By default, telegraf gathers stats from any up interface (excluding loopback)
#   ## Setting interfaces will tell it to gather these explicit interfaces,
#   ## regardless of status.
#   ##
#   # interfaces = ["eth0"]


# # TCP or UDP 'ping' given url and collect response time in seconds
# [[inputs.net_response]]
#   ## Protocol, must be "tcp" or "udp"
#   protocol = "tcp"
#   ## Server address (default localhost)
#   address = "github.com:80"
#   ## Set timeout
#   timeout = "1s"
#
#   ## Optional string sent to the server
#   # send = "ssh"
#   ## Optional expected string in answer
#   # expect = "ssh"
#   ## Set read timeout (only used if expecting a response)
#   read_timeout = "1s"


# # Read TCP metrics such as established, time wait and sockets counts.
# [[inputs.netstat]]
#   # no configuration


# # Read Nginx's basic status information (ngx_http_stub_status_module)
# [[inputs.nginx]]
#   ## An array of Nginx stub_status URI to gather stats.
#   urls = ["http://localhost/status"]


# # Read NSQ topic and channel statistics.
# [[inputs.nsq]]
#   ## An array of NSQD HTTP API endpoints
#   endpoints = ["http://localhost:4151"]


# # Get standard NTP query metrics, requires ntpq executable.
# [[inputs.ntpq]]
#   ## If false, set the -n ntpq flag. Can reduce metric gather time.
#   dns_lookup = true


# # Read metrics from one or many pgbouncer servers
# [[inputs.pgbouncer]]
#   ## specify address via a url matching:
#   ##   postgres://[pqgotest[:password]]@localhost:port[/dbname]\
#   ##       ?sslmode=[disable|verify-ca|verify-full]
#   ## or a simple string:
#   ##   host=localhost user=pqotest port=6432 password=... sslmode=... dbname=pgbouncer
#   ##
#   ## All connection parameters are optional, except for dbname,
#   ## you need to set it always as pgbouncer.
#   address = "host=localhost user=postgres port=6432 sslmode=disable dbname=pgbouncer"
#
#   ## A list of databases to pull metrics about. If not specified, metrics for all
#   ## databases are gathered.
#   # databases = ["app_production", "testing"]


# # Read metrics of phpfpm, via HTTP status page or socket
# [[inputs.phpfpm]]
#   ## An array of addresses to gather stats about. Specify an ip or hostname
#   ## with optional port and path
#   ##
#   ## Plugin can be configured in three modes (either can be used):
#   ##   - http: the URL must start with http:// or https://, ie:
#   ##       "http://localhost/status"
#   ##       "http://192.168.130.1/status?full"
#   ##
#   ##   - unixsocket: path to fpm socket, ie:
#   ##       "/var/run/php5-fpm.sock"
#   ##      or using a custom fpm status path:
#   ##       "/var/run/php5-fpm.sock:fpm-custom-status-path"
#   ##
#   ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:
#   ##       "fcgi://10.0.0.12:9000/status"
#   ##       "cgi://10.0.10.12:9001/status"
#   ##
#   ## Example of multiple gathering from local socket and remove host
#   ## urls = ["http://192.168.1.20/status", "/tmp/fpm.sock"]
#   urls = ["http://localhost/status"]


# # Ping given url(s) and return statistics
# [[inputs.ping]]
#   ## NOTE: this plugin forks the ping command. You may need to set capabilities
#   ## via setcap cap_net_raw+p /bin/ping
#   #
#   ## urls to ping
#   urls = ["www.google.com"] # required
#   ## number of pings to send per collection (ping -c <COUNT>)
#   count = 1 # required
#   ## interval, in s, at which to ping. 0 == default (ping -i <PING_INTERVAL>)
#   ping_interval = 0.0
#   ## per-ping timeout, in s. 0 == no timeout (ping -W <TIMEOUT>)
#   timeout = 1.0
#   ## interface to send ping from (ping -I <INTERFACE>)
#   interface = ""


# # Read metrics from one or many postgresql servers
# [[inputs.postgresql]]
#   ## specify address via a url matching:
#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\
#   ##       ?sslmode=[disable|verify-ca|verify-full]
#   ## or a simple string:
#   ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production
#   ##
#   ## All connection parameters are optional.
#   ##
#   ## Without the dbname parameter, the driver will default to a database
#   ## with the same name as the user. This dbname is just for instantiating a
#   ## connection with the server and doesn't restrict the databases we are trying
#   ## to grab metrics for.
#   ##
#   address = "host=localhost user=postgres sslmode=disable"
#
#   ## A list of databases to pull metrics about. If not specified, metrics for all
#   ## databases are gathered.
#   # databases = ["app_production", "testing"]


# # Read metrics from one or many postgresql servers
# [[inputs.postgresql_extensible]]
#   ## specify address via a url matching:
#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\
#   ##       ?sslmode=[disable|verify-ca|verify-full]
#   ## or a simple string:
#   ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production
#   #
#   ## All connection parameters are optional.  #
#   ## Without the dbname parameter, the driver will default to a database
#   ## with the same name as the user. This dbname is just for instantiating a
#   ## connection with the server and doesn't restrict the databases we are trying
#   ## to grab metrics for.
#   #
#   address = "host=localhost user=postgres sslmode=disable"
#   ## A list of databases to pull metrics about. If not specified, metrics for all
#   ## databases are gathered.
#   ## databases = ["app_production", "testing"]
#   #
#   # outputaddress = "db01"
#   ## A custom name for the database that will be used as the "server" tag in the
#   ## measurement output. If not specified, a default one generated from
#   ## the connection address is used.
#   #
#   ## Define the toml config where the sql queries are stored
#   ## New queries can be added, if the withdbname is set to true and there is no
#   ## databases defined in the 'databases field', the sql query is ended by a
#   ## 'is not null' in order to make the query succeed.
#   ## Example :
#   ## The sqlquery : "SELECT * FROM pg_stat_database where datname" become
#   ## "SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')"
#   ## because the databases variable was set to ['postgres', 'pgbench' ] and the
#   ## withdbname was true. Be careful that if the withdbname is set to false you
#   ## don't have to define the where clause (aka with the dbname) the tagvalue
#   ## field is used to define custom tags (separated by commas)
#   ## The optional "measurement" value can be used to override the default
#   ## output measurement name ("postgresql").
#   #
#   ## Structure :
#   ## [[inputs.postgresql_extensible.query]]
#   ##   sqlquery string
#   ##   version string
#   ##   withdbname boolean
#   ##   tagvalue string (comma separated)
#   ##   measurement string
#   [[inputs.postgresql_extensible.query]]
#     sqlquery="SELECT * FROM pg_stat_database"
#     version=901
#     withdbname=false
#     tagvalue=""
#     measurement=""
#   [[inputs.postgresql_extensible.query]]
#     sqlquery="SELECT * FROM pg_stat_bgwriter"
#     version=901
#     withdbname=false
#     tagvalue="postgresql.stats"


# # A plugin to collect stats from Varnish HTTP Cache
# [[inputs.varnish]]
#   ## The default location of the varnishstat binary can be overridden with:
#   binary = "/usr/bin/varnishstat"
#
#   ## By default, telegraf gather stats for 3 metric points.
#   ## Setting stats will override the defaults shown below.
#   ## Glob matching can be used, ie, stats = ["MAIN.*"]
#   ## stats may also be set to ["*"], which will collect all stats
#   stats = ["MAIN.cache_hit", "MAIN.cache_miss", "MAIN.uptime"]


###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################